{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression & Its Evaluation"
      ],
      "metadata": {
        "id": "oUtwZkeP5582"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: What is Simple Linear Regression?\n",
        "   - Simple Linear Regression\n",
        "     Introduction\n",
        "\n",
        "     Simple Linear Regression (SLR) is one of the most fundamental and widely used statistical techniques for modeling the relationship between two quantitative variables. It helps in predicting the value of one variable, known as the dependent variable (Y), based on the value of another variable, known as the independent variable (X). The term â€œlinearâ€ indicates that the relationship between the variables can be represented by a straight line.\n",
        "\n",
        "     In other words, Simple Linear Regression aims to find the best-fitting straight line that describes how the dependent variable changes as the independent variable changes.\n",
        "\n",
        "     *Definition\n",
        "\n",
        "     Simple Linear Regression is a statistical method that models the relationship between two variables by fitting a linear equation to observed data. The general form of the simple linear regression equation is:\n",
        "\n",
        "      ğ‘Œ=ğ‘+ğ‘ğ‘‹+ğ‘’  Y=a+bX+e\n",
        "\n",
        "     Where:\n",
        "\n",
        "     Y = Dependent variable (the variable we want to predict)\n",
        "\n",
        "     X = Independent variable (the predictor variable)\n",
        "\n",
        "     a = Intercept (the value of Y when X = 0)\n",
        "\n",
        "     b = Slope (the change in Y for a one-unit change in X)\n",
        "\n",
        "     e = Error term or residual (the difference between observed and predicted values)\n",
        "\n",
        "     *Objective of Simple Linear Regression\n",
        "\n",
        "     The main objectives of Simple Linear Regression are:\n",
        "\n",
        "     To establish a relationship between two variables.\n",
        "\n",
        "     To predict the value of one variable based on the other.\n",
        "\n",
        "     To analyze the strength and direction of the relationship.\n",
        "\n",
        "     To minimize the error between observed and predicted values.\n",
        "\n",
        "     *Assumptions of Simple Linear Regression\n",
        "\n",
        "     For the results of regression analysis to be valid, the following assumptions must hold true:\n",
        "\n",
        "     Linearity: The relationship between X and Y must be linear.\n",
        "\n",
        "     Independence: The residuals (errors) should be independent of each other.\n",
        "\n",
        "     Homoscedasticity: The variance of residuals should be constant across all values of X.\n",
        "\n",
        "     Normality: The residuals should be normally distributed.\n",
        "\n",
        "     No multicollinearity: (Since SLR has only one predictor, this is automatically satisfied.)\n",
        "\n",
        "     *Method of Estimation\n",
        "\n",
        "     The most common method used to estimate the regression line is the Least Squares Method.\n",
        "     According to this method, the line of best fit minimizes the sum of the squared differences between the observed values and the predicted values of Y. Mathematically, we minimize:\n",
        "\n",
        "     Minimize\n",
        "     âˆ‘(ğ‘Œğ‘–âˆ’ğ‘Œğ‘–^)2\n",
        "     Minimize âˆ‘(Yi âˆ’ Yi ^)2\n",
        "\n",
        "     Where:\n",
        "     ğ‘Œğ‘– = Observed value\n",
        "     ğ‘Œğ‘–^ = Predicted value\n",
        "\n",
        "\n",
        "     The slope (b) and intercept (a) are computed using the following formulas:\n",
        "\n",
        "     ğ‘›(âˆ‘ğ‘‹ğ‘Œ) âˆ’ (âˆ‘ğ‘‹)(âˆ‘ğ‘Œ)\n",
        "b =  __________________\n",
        "     ğ‘›(âˆ‘ğ‘‹2) âˆ’ (âˆ‘ğ‘‹)2\n",
        "b=\n",
        "n(âˆ‘X\n",
        "2\n",
        ")âˆ’(âˆ‘X)\n",
        "2\n",
        "n(âˆ‘XY)âˆ’(âˆ‘X)(âˆ‘Y)\n",
        "\tâ€‹\n",
        "\n",
        "ğ‘\n",
        "=\n",
        "ğ‘Œ\n",
        "Ë‰\n",
        "âˆ’\n",
        "ğ‘\n",
        "ğ‘‹\n",
        "Ë‰\n",
        "a=\n",
        "Y\n",
        "Ë‰\n",
        "âˆ’b\n",
        "X\n",
        "Ë‰\n",
        "\n",
        "Where\n",
        "ğ‘‹\n",
        "Ë‰\n",
        "X\n",
        "Ë‰\n",
        " and\n",
        "ğ‘Œ\n",
        "Ë‰\n",
        "Y\n",
        "Ë‰\n",
        " are the means of X and Y respectively.\n",
        "\n",
        "6. Interpretation of the Regression Line\n",
        "\n",
        "Intercept (a): Indicates the expected value of Y when X = 0.\n",
        "\n",
        "Slope (b): Represents the average change in Y for each unit increase in X.\n",
        "If\n",
        "ğ‘\n",
        ">\n",
        "0\n",
        "b>0, the relationship is positive (Y increases with X).\n",
        "If\n",
        "ğ‘\n",
        "<\n",
        "0\n",
        "b<0, the relationship is negative (Y decreases with X).\n",
        "\n",
        "7. Coefficient of Determination (RÂ²)\n",
        "\n",
        "The R-squared (RÂ²) value measures how well the regression line fits the data. It is the proportion of the variance in the dependent variable that can be explained by the independent variable.\n",
        "\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "ğ¸\n",
        "ğ‘¥\n",
        "ğ‘\n",
        "ğ‘™\n",
        "ğ‘\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "ğ‘’\n",
        "ğ‘‘\n",
        "\n",
        "ğ‘‰\n",
        "ğ‘\n",
        "ğ‘Ÿ\n",
        "ğ‘–\n",
        "ğ‘\n",
        "ğ‘¡\n",
        "ğ‘–\n",
        "ğ‘œ\n",
        "ğ‘›\n",
        "ğ‘‡\n",
        "ğ‘œ\n",
        "ğ‘¡\n",
        "ğ‘\n",
        "ğ‘™\n",
        "\n",
        "ğ‘‰\n",
        "ğ‘\n",
        "ğ‘Ÿ\n",
        "ğ‘–\n",
        "ğ‘\n",
        "ğ‘¡\n",
        "ğ‘–\n",
        "ğ‘œ\n",
        "ğ‘›\n",
        "R\n",
        "2\n",
        "=\n",
        "Total Variation\n",
        "Explained Variation\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "The value of RÂ² ranges between 0 and 1:\n",
        "\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "1\n",
        "R\n",
        "2\n",
        "=1: Perfect fit\n",
        "\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "0\n",
        "R\n",
        "2\n",
        "=0: No relationship\n",
        "\n",
        "8. Applications of Simple Linear Regression\n",
        "\n",
        "Economics: Forecasting demand, sales, and production.\n",
        "\n",
        "Finance: Predicting stock prices or returns.\n",
        "\n",
        "Engineering: Estimating system performance or energy consumption.\n",
        "\n",
        "Healthcare: Predicting disease progression based on a measurable factor.\n",
        "\n",
        "Education: Estimating student performance from attendance or study hours.\n",
        "\n",
        "9. Example\n",
        "\n",
        "Suppose we want to study the relationship between hours studied (X) and marks obtained (Y) by students.\n",
        "If we collect data and derive the equation:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "20\n",
        "+\n",
        "5\n",
        "ğ‘‹\n",
        "Y=20+5X\n",
        "\n",
        "This means:\n",
        "\n",
        "When a student studies 0 hours, the expected marks are 20.\n",
        "\n",
        "For every additional hour studied, the marks increase by 5.\n",
        "\n",
        "Thus, a student studying 6 hours is expected to score:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "20\n",
        "+\n",
        "5\n",
        "(\n",
        "6\n",
        ")\n",
        "=\n",
        "50\n",
        "Y=20+5(6)=50\n",
        "10. Advantages\n",
        "\n",
        "Easy to understand and interpret.\n",
        "\n",
        "Useful for making predictions.\n",
        "\n",
        "Requires less computational effort.\n",
        "\n",
        "Provides a basis for more complex models like multiple regression.\n",
        "\n",
        "11. Limitations\n",
        "\n",
        "Works only when the relationship is linear.\n",
        "\n",
        "Sensitive to outliers.\n",
        "\n",
        "Affected by violation of assumptions.\n",
        "\n",
        "Cannot handle multiple independent variables.\n",
        "\n",
        "12. Conclusion\n",
        "\n",
        "Simple Linear Regression is a powerful yet straightforward tool in statistical analysis and machine learning. It serves as the foundation for understanding complex predictive models. By identifying a linear relationship between two variables, it enables effective forecasting and decision-making across various fields such as business, science, and engineering."
      ],
      "metadata": {
        "id": "WNv_PwNs6BS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: What are the key assumptions of Simple Linear Regression?\n",
        "   -Key Assumptions of Simple Linear Regression\n",
        "1. Introduction\n",
        "\n",
        "Simple Linear Regression (SLR) is a statistical technique used to study the relationship between two quantitative variables â€” one independent (predictor) and one dependent (response) variable.\n",
        "The goal of regression analysis is to find the best-fitting straight line that explains how the dependent variable (Y) changes with the independent variable (X).\n",
        "\n",
        "However, for the regression results to be valid, reliable, and meaningful, certain statistical assumptions must be satisfied. These assumptions ensure that the model accurately represents the data and that the estimated parameters (slope and intercept) are unbiased and efficient.\n",
        "\n",
        "2. Meaning of Assumptions\n",
        "\n",
        "An assumption in statistics is a condition or rule that must hold true for the chosen analytical method to work correctly.\n",
        "In the context of Simple Linear Regression, assumptions are the basic requirements regarding the nature of data, relationship between variables, and the behavior of residuals (errors).\n",
        "\n",
        "3. The Key Assumptions of Simple Linear Regression\n",
        "\n",
        "There are five major assumptions that form the foundation of Simple Linear Regression. Each assumption is explained below in detail.\n",
        "\n",
        "3.1 Linearity of Relationship\n",
        "\n",
        "The first and most important assumption of Simple Linear Regression is that there is a linear relationship between the independent variable (X) and the dependent variable (Y).\n",
        "This means that the change in Y is directly proportional to the change in X. In mathematical form, the model assumes:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "+\n",
        "ğ‘\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘’\n",
        "Y=a+bX+e\n",
        "\n",
        "where a is the intercept, b is the slope, and e is the error term.\n",
        "\n",
        "If the actual relationship is non-linear (for example, curved or exponential), the linear regression model will not fit the data well, leading to incorrect predictions.\n",
        "\n",
        "Example:\n",
        "If the relationship between hours studied and exam marks is roughly straight-line increasing, then the linearity assumption holds true.\n",
        "\n",
        "3.2 Independence of Errors\n",
        "\n",
        "The second assumption is that the residuals (errors) â€” the differences between the observed and predicted values â€” are independent of each other.\n",
        "This means that the error for one observation should not influence or depend on the error for another observation.\n",
        "\n",
        "Violation of this assumption (known as autocorrelation) often occurs in time-series data, where consecutive observations are correlated.\n",
        "\n",
        "Test for this assumption:\n",
        "The Durbin-Watson test is commonly used to detect autocorrelation.\n",
        "\n",
        "Example:\n",
        "In predicting stock prices over time, todayâ€™s price might depend on yesterdayâ€™s price, violating the independence assumption.\n",
        "\n",
        "3.3 Homoscedasticity (Constant Variance of Errors)\n",
        "\n",
        "The third assumption states that the variance of the residuals should be constant across all values of X.\n",
        "This condition is known as Homoscedasticity.\n",
        "\n",
        "If the spread of errors increases or decreases with X (a pattern called Heteroscedasticity), the modelâ€™s predictions become unreliable, and standard errors may be biased.\n",
        "\n",
        "Graphical Check:\n",
        "A residual plot (residuals vs. fitted values) should show a random scatter without any clear pattern or funnel shape.\n",
        "\n",
        "Example:\n",
        "If predicting income from years of experience, the variability in income should be roughly the same for all experience levels.\n",
        "\n",
        "3.4 Normality of Errors\n",
        "\n",
        "The fourth assumption is that the residuals (error terms) are normally distributed.\n",
        "This means when plotted, the residuals should form a bell-shaped normal distribution curve centered around zero.\n",
        "\n",
        "This assumption is especially important when conducting hypothesis tests and building confidence intervals for regression coefficients.\n",
        "\n",
        "Methods to check:\n",
        "\n",
        "Histogram of residuals should look bell-shaped.\n",
        "\n",
        "Q-Q plot should show points lying approximately on a straight line.\n",
        "\n",
        "Example:\n",
        "If the model errors are not normally distributed (e.g., skewed), it may affect the accuracy of significance tests.\n",
        "\n",
        "3.5 No Measurement Error in the Independent Variable\n",
        "\n",
        "Another assumption is that the independent variable (X) is measured accurately without error.\n",
        "If X values contain large measurement errors, it can bias the estimated slope (b) and reduce the accuracy of predictions.\n",
        "The dependent variable (Y), however, is allowed to have random error.\n",
        "\n",
        "Example:\n",
        "If X represents temperature readings taken by a faulty thermometer, this assumption is violated.\n",
        "\n",
        "3.6 No Multicollinearity (for Multiple Regression)\n",
        "\n",
        "Although Simple Linear Regression deals with only one independent variable, it is worth noting that multicollinearity becomes an assumption in Multiple Linear Regression.\n",
        "In SLR, since there is only one predictor, this condition is automatically satisfied.\n",
        "\n",
        "4. Importance of These Assumptions\n",
        "\n",
        "If these assumptions are satisfied:\n",
        "\n",
        "The regression estimates (slope and intercept) are unbiased.\n",
        "\n",
        "Predictions made by the model are accurate and reliable.\n",
        "\n",
        "Statistical tests such as t-tests and F-tests give valid results.\n",
        "\n",
        "If assumptions are violated:\n",
        "\n",
        "The model may produce biased estimates.\n",
        "\n",
        "The confidence intervals and p-values become misleading.\n",
        "\n",
        "The overall predictive power of the regression decreases.\n",
        "\n",
        "5. Diagnostic Tools to Check Assumptions\n",
        "\n",
        "Scatter Plot â€“ to check linearity.\n",
        "\n",
        "Residual Plot â€“ to check independence and homoscedasticity.\n",
        "\n",
        "Histogram or Q-Q Plot â€“ to check normality of residuals.\n",
        "\n",
        "Durbin-Watson Test â€“ to check for autocorrelation.\n",
        "\n",
        "6. Example Summary\n",
        "\n",
        "Suppose a researcher wants to predict sales (Y) based on advertising expenditure (X) using Simple Linear Regression.\n",
        "Before fitting the model, they should check:\n",
        "\n",
        "Sales and advertising cost have a linear relationship.\n",
        "\n",
        "Errors are independent and random.\n",
        "\n",
        "The variance of errors remains constant across different spending levels.\n",
        "\n",
        "Errors are normally distributed.\n",
        "\n",
        "If these conditions hold, the regression model can accurately predict future sales.\n",
        "\n",
        "7. Conclusion\n",
        "\n",
        "In conclusion, the key assumptions of Simple Linear Regression form the foundation for its successful application. These assumptions â€” linearity, independence, homoscedasticity, normality, and accurate measurement of variables â€” ensure that the model is statistically sound and the results are meaningful.\n",
        "Violation of these assumptions can lead to incorrect interpretations and unreliable predictions. Therefore, it is essential to test and verify these assumptions before relying on regression outcomes."
      ],
      "metadata": {
        "id": "5bGSmfMz_VNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xM47Se0_Oos-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: What is heteroscedasticity, and why is it important to address in regression\n",
        "models?\n",
        " -Heteroscedasticity and Its Importance in Regression Models\n",
        "1. Introduction\n",
        "\n",
        "In regression analysis, one of the key assumptions of the Classical Linear Regression Model (CLRM) is that the variance of the error terms (residuals) remains constant across all levels of the independent variable(s). This property is known as Homoscedasticity.\n",
        "\n",
        "When this assumption is violated â€” meaning the variance of the residuals is not constant â€” the condition is called Heteroscedasticity.\n",
        "Heteroscedasticity can distort the results of regression analysis, making statistical inferences unreliable. Therefore, detecting and correcting it is an important step in building an accurate regression model.\n",
        "\n",
        "2. Definition\n",
        "\n",
        "Heteroscedasticity refers to the situation in a regression model where the variance of the error terms changes with the values of the independent variable(s).\n",
        "\n",
        "In mathematical terms, the assumption of constant variance is:\n",
        "\n",
        "ğ‘‰\n",
        "ğ‘\n",
        "ğ‘Ÿ\n",
        "(\n",
        "ğ‘’\n",
        "ğ‘–\n",
        ")\n",
        "=\n",
        "ğœ\n",
        "2\n",
        "for all\n",
        "ğ‘–\n",
        "Var(e\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")=Ïƒ\n",
        "2\n",
        "for all i\n",
        "\n",
        "If instead:\n",
        "\n",
        "ğ‘‰\n",
        "ğ‘\n",
        "ğ‘Ÿ\n",
        "(\n",
        "ğ‘’\n",
        "ğ‘–\n",
        ")\n",
        "â‰ \n",
        "ğœ\n",
        "2\n",
        "Var(e\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "î€ \n",
        "=Ïƒ\n",
        "2\n",
        "\n",
        "then heteroscedasticity is present.\n",
        "\n",
        "This means that some observations have larger or smaller spread of errors than others â€” the residuals are not evenly distributed.\n",
        "\n",
        "3. Graphical Representation\n",
        "\n",
        "A simple way to visualize heteroscedasticity is through a residual plot.\n",
        "\n",
        "In a homoscedastic model, residuals are scattered randomly around zero with roughly equal spread.\n",
        "\n",
        "In a heteroscedastic model, residuals form a pattern â€” for example, a cone shape where the spread increases or decreases with X.\n",
        "\n",
        "Example Diagram (conceptual):\n",
        "Homoscedasticity:     Heteroscedasticity:\n",
        "|                     /\n",
        "|   * . * . * . *    / * . * . * . *\n",
        "|___.___.___.___.   /___.___.___.___.\n",
        "\n",
        "4. Causes of Heteroscedasticity\n",
        "\n",
        "Heteroscedasticity often occurs in real-world data due to a variety of reasons, such as:\n",
        "\n",
        "Scale effects: When the magnitude of the dependent variable increases with the independent variable.\n",
        "Example: Higher income groups show more variability in spending.\n",
        "\n",
        "Model misspecification: Omission of important variables or incorrect functional form.\n",
        "\n",
        "Presence of outliers: Extreme values in data can distort error variance.\n",
        "\n",
        "Data from different sub-populations: If the sample includes diverse groups with different characteristics (e.g., countries, industries).\n",
        "\n",
        "Measurement errors: Inaccurate data collection can cause unequal variances.\n",
        "\n",
        "5. Consequences of Heteroscedasticity\n",
        "\n",
        "Heteroscedasticity does not bias the regression coefficients themselves, but it affects their reliability.\n",
        "The main consequences include:\n",
        "\n",
        "Inefficient Estimates:\n",
        "The Ordinary Least Squares (OLS) estimator remains unbiased but becomes inefficient, meaning it no longer has the minimum variance among all unbiased estimators.\n",
        "\n",
        "Incorrect Standard Errors:\n",
        "Standard errors of coefficients are wrongly estimated, leading to unreliable t-tests and F-tests.\n",
        "\n",
        "Invalid Hypothesis Testing:\n",
        "Because of incorrect standard errors, the calculated p-values may be misleading, causing false conclusions about significance.\n",
        "\n",
        "Poor Confidence Intervals:\n",
        "Confidence intervals for the parameters may become too wide or too narrow.\n",
        "\n",
        "Reduced Predictive Power:\n",
        "The modelâ€™s predictions may become unstable for certain ranges of the data.\n",
        "\n",
        "6. Detection of Heteroscedasticity\n",
        "\n",
        "There are both graphical and statistical methods to detect heteroscedasticity.\n",
        "\n",
        "A. Graphical Methods\n",
        "\n",
        "Residual Plot: Plot residuals vs. fitted values â€” a pattern indicates heteroscedasticity.\n",
        "\n",
        "Scale-Location Plot: Shows spread of residuals; an increasing or decreasing trend suggests heteroscedasticity.\n",
        "\n",
        "B. Statistical Tests\n",
        "\n",
        "Breuschâ€“Pagan Test: Checks if residual variance depends on independent variables.\n",
        "\n",
        "Whiteâ€™s Test: Detects any general form of heteroscedasticity.\n",
        "\n",
        "Goldfeldâ€“Quandt Test: Divides data into two groups and compares variances.\n",
        "\n",
        "7. Remedies for Heteroscedasticity\n",
        "\n",
        "Several methods can be applied to correct or minimize the effects of heteroscedasticity:\n",
        "\n",
        "Transforming Variables:\n",
        "\n",
        "Apply logarithmic, square root, or Boxâ€“Cox transformation to stabilize variance.\n",
        "Example: Using log(Y) instead of Y.\n",
        "\n",
        "Weighted Least Squares (WLS):\n",
        "Assigns weights to observations inversely proportional to their variances, giving less weight to observations with large residuals.\n",
        "\n",
        "Robust Standard Errors:\n",
        "Also known as Heteroscedasticity-Consistent Standard Errors (HCSE), these adjust the standard errors without changing the regression coefficients.\n",
        "\n",
        "Model Re-specification:\n",
        "Include omitted variables or use a more appropriate functional form if model misspecification is the cause.\n",
        "\n",
        "8. Example\n",
        "\n",
        "Consider a regression model that predicts monthly household expenditure (Y) based on income (X).\n",
        "\n",
        "For low-income families, spending varies little; for high-income families, spending varies greatly.\n",
        "Thus, as income increases, the variance of spending also increases â€” this is a clear case of heteroscedasticity.\n",
        "\n",
        "If not corrected, the model might wrongly suggest that income has a smaller or larger effect on expenditure than it truly does.\n",
        "\n",
        "9. Importance of Addressing Heteroscedasticity\n",
        "\n",
        "Addressing heteroscedasticity is crucial because:\n",
        "\n",
        "It ensures that the OLS estimators are efficient (minimum variance).\n",
        "\n",
        "It provides valid hypothesis testing results.\n",
        "\n",
        "It improves accuracy of confidence intervals and predictions.\n",
        "\n",
        "It enhances the reliability and interpretability of regression outcomes.\n",
        "\n",
        "It avoids misleading conclusions in research and decision-making.\n",
        "\n",
        "10. Conclusion\n",
        "\n",
        "In conclusion, heteroscedasticity is a serious issue in regression analysis where the variance of errors changes across observations. While it does not bias the estimated coefficients, it makes them inefficient and leads to incorrect inferences.\n",
        "\n",
        "Detecting and correcting heteroscedasticity is therefore essential for achieving accurate, valid, and trustworthy regression results.\n",
        "Using appropriate transformations, robust estimation methods, or model re-specification can help ensure that the regression model remains statistically sound."
      ],
      "metadata": {
        "id": "LO-eZQMmEmVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: What is Multiple Linear Regression?\n",
        " -Multiple Linear Regression\n",
        "1. Introduction\n",
        "\n",
        "Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that allows us to study and model the relationship between one dependent variable and two or more independent variables. It is one of the most commonly used statistical techniques in data analysis, econometrics, machine learning, and scientific research.\n",
        "\n",
        "While Simple Linear Regression considers only one factor affecting the outcome, Multiple Linear Regression helps to understand how multiple factors together influence the dependent variable. It provides a more realistic and accurate model of complex real-world relationships.\n",
        "\n",
        "2. Definition\n",
        "\n",
        "Multiple Linear Regression (MLR) is a statistical technique used to model the relationship between a single dependent (response) variable and two or more independent (predictor) variables by fitting a linear equation to observed data.\n",
        "\n",
        "The general form of the Multiple Linear Regression model is:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "1\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "ğ‘\n",
        "3\n",
        "ğ‘‹\n",
        "3\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "+\n",
        "ğ‘’\n",
        "Y=a+b\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "+b\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "+b\n",
        "3\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "3\n",
        "\tâ€‹\n",
        "\n",
        "+â‹¯+b\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "+e\n",
        "\n",
        "Where:\n",
        "\n",
        "Y = Dependent variable (the variable being predicted)\n",
        "\n",
        "Xâ‚, Xâ‚‚, Xâ‚ƒ, â€¦, Xâ‚™ = Independent variables (predictors)\n",
        "\n",
        "a = Intercept (value of Y when all Xâ€™s are zero)\n",
        "\n",
        "bâ‚, bâ‚‚, bâ‚ƒ, â€¦, bâ‚™ = Regression coefficients (representing the change in Y due to a one-unit change in X while keeping others constant)\n",
        "\n",
        "e = Error term (difference between observed and predicted values)\n",
        "\n",
        "3. Objective of Multiple Linear Regression\n",
        "\n",
        "The main objectives of MLR are:\n",
        "\n",
        "To describe how multiple independent variables are related to the dependent variable.\n",
        "\n",
        "To estimate and interpret the impact of each independent variable while controlling for others.\n",
        "\n",
        "To predict the value of the dependent variable based on known values of independent variables.\n",
        "\n",
        "To assess the relative importance of different predictors.\n",
        "\n",
        "4. Example\n",
        "\n",
        "Suppose a company wants to predict sales (Y) based on advertising expenditure (Xâ‚), price of product (Xâ‚‚), and number of salespeople (Xâ‚ƒ).\n",
        "\n",
        "The regression equation might look like:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "5000\n",
        "+\n",
        "2.3\n",
        "ğ‘‹\n",
        "1\n",
        "âˆ’\n",
        "1.8\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "0.5\n",
        "ğ‘‹\n",
        "3\n",
        "Y=5000+2.3X\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’1.8X\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "+0.5X\n",
        "3\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "The intercept (5000) represents the expected sales when all independent variables are zero.\n",
        "\n",
        "The coefficient 2.3 means that for each additional unit of advertising expenditure, sales increase by 2.3 units, assuming price and number of salespeople remain constant.\n",
        "\n",
        "The coefficient -1.8 indicates that as the product price increases by one unit, sales decrease by 1.8 units, keeping other variables constant.\n",
        "\n",
        "5. Assumptions of Multiple Linear Regression\n",
        "\n",
        "For the MLR results to be valid, several assumptions must be satisfied:\n",
        "\n",
        "Linearity: The relationship between dependent and independent variables is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals is constant across all levels of predictors.\n",
        "\n",
        "Normality: The residuals are normally distributed.\n",
        "\n",
        "No Multicollinearity: Independent variables should not be highly correlated with each other.\n",
        "\n",
        "No Autocorrelation: Error terms should not be correlated with each other.\n",
        "\n",
        "6. Estimation of Parameters\n",
        "\n",
        "The parameters (intercept and coefficients) in Multiple Linear Regression are estimated using the Ordinary Least Squares (OLS) method.\n",
        "\n",
        "OLS minimizes the sum of squared residuals:\n",
        "\n",
        "Minimize\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘Œ\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘Œ\n",
        "ğ‘–\n",
        "^\n",
        ")\n",
        "2\n",
        "Minimize âˆ‘(Y\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’\n",
        "Y\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "^\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘Œ\n",
        "ğ‘–\n",
        "^\n",
        "=\n",
        "ğ‘\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "1\n",
        "ğ‘–\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "ğ‘–\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "Y\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "^\n",
        "\tâ€‹\n",
        "\n",
        "=a+b\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "1i\n",
        "\tâ€‹\n",
        "\n",
        "+b\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2i\n",
        "\tâ€‹\n",
        "\n",
        "+â‹¯+b\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "ni\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "The OLS method ensures that the estimated regression line provides the best fit to the observed data.\n",
        "\n",
        "7. Interpretation of Coefficients\n",
        "\n",
        "Each regression coefficient represents the partial effect of an independent variable on the dependent variable, holding all other variables constant.\n",
        "\n",
        "Positive coefficient: Indicates a direct relationship â€” as X increases, Y increases.\n",
        "\n",
        "Negative coefficient: Indicates an inverse relationship â€” as X increases, Y decreases.\n",
        "\n",
        "This â€œholding other variables constantâ€ concept is crucial in multiple regression because it isolates the individual impact of each predictor.\n",
        "\n",
        "8. Coefficient of Determination (RÂ²)\n",
        "\n",
        "The Coefficient of Determination (RÂ²) measures how well the independent variables collectively explain the variation in the dependent variable.\n",
        "\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "Explained Variation\n",
        "Total Variation\n",
        "R\n",
        "2\n",
        "=\n",
        "Total Variation\n",
        "Explained Variation\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "1\n",
        "R\n",
        "2\n",
        "=1: Perfect prediction\n",
        "\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "0\n",
        "R\n",
        "2\n",
        "=0: No relationship between X and Y\n",
        "\n",
        "In MLR, the Adjusted RÂ² is often used since it accounts for the number of predictors in the model.\n",
        "\n",
        "9. Advantages of Multiple Linear Regression\n",
        "\n",
        "Handles multiple independent variables simultaneously.\n",
        "\n",
        "Provides deeper insight into relationships between variables.\n",
        "\n",
        "Useful for prediction and forecasting.\n",
        "\n",
        "Identifies the relative importance of each predictor.\n",
        "\n",
        "Allows control for confounding variables (isolating effects).\n",
        "\n",
        "10. Limitations of Multiple Linear Regression\n",
        "\n",
        "Assumes linear relationships, which may not always exist.\n",
        "\n",
        "Sensitive to outliers and multicollinearity.\n",
        "\n",
        "Requires large sample size for reliable estimates.\n",
        "\n",
        "Violations of assumptions can lead to biased or inefficient results.\n",
        "\n",
        "Interpretation becomes difficult with many variables.\n",
        "\n",
        "11. Applications of Multiple Linear Regression\n",
        "\n",
        "Multiple Linear Regression is used in various fields, including:\n",
        "\n",
        "Economics: Forecasting GDP, inflation, and market trends.\n",
        "\n",
        "Finance: Predicting stock prices, investment returns, and credit risk.\n",
        "\n",
        "Healthcare: Studying the effect of diet, exercise, and age on patient outcomes.\n",
        "\n",
        "Education: Analyzing how attendance, study time, and teaching methods affect grades.\n",
        "\n",
        "Engineering: Estimating product quality based on material and design variables.\n",
        "\n",
        "12. Importance of Multiple Linear Regression\n",
        "\n",
        "Multiple Linear Regression is important because it provides a comprehensive understanding of complex relationships where several factors influence an outcome. It helps in decision-making, forecasting, and identifying key drivers of performance. It is also the basis for more advanced models like logistic regression, ridge regression, and machine learning algorithms.\n",
        "\n",
        "13. Conclusion\n",
        "\n",
        "In conclusion, Multiple Linear Regression is a fundamental and powerful statistical tool that extends the simple regression concept to multiple predictors. It enables researchers and analysts to understand and quantify the effect of several factors simultaneously on a single outcome.\n",
        "When properly applied with its assumptions satisfied, MLR serves as an effective method for prediction, explanation, and data-driven decision-making."
      ],
      "metadata": {
        "id": "tNbZZFV9FfKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: What is polynomial regression, and how does it differ from linear\n",
        "regression?\n",
        " -Polynomial Regression and Its Difference from Linear Regression\n",
        "1. Introduction\n",
        "\n",
        "Regression analysis is a key statistical method used to model the relationship between a dependent variable and one or more independent variables.\n",
        "While Linear Regression assumes a straight-line relationship between the variables, Polynomial Regression extends this concept to model non-linear relationships using polynomial equations.\n",
        "\n",
        "Polynomial Regression is particularly useful when data shows a curved pattern that a straight line cannot adequately represent.\n",
        "\n",
        "2. Definition of Polynomial Regression\n",
        "\n",
        "Polynomial Regression is a type of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial.\n",
        "\n",
        "It is used when the data follows a non-linear trend but can still be approximated by a polynomial curve.\n",
        "\n",
        "The general form of a Polynomial Regression equation is:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "ğ‘\n",
        "3\n",
        "ğ‘‹\n",
        "3\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "+\n",
        "ğ‘’\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+a\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "+a\n",
        "3\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "3\n",
        "+â‹¯+a\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "n\n",
        "+e\n",
        "\n",
        "Where:\n",
        "\n",
        "Y = Dependent variable\n",
        "\n",
        "X = Independent variable\n",
        "\n",
        "aâ‚€ = Intercept (value of Y when X = 0)\n",
        "\n",
        "aâ‚, aâ‚‚, â€¦, aâ‚™ = Coefficients of the polynomial terms\n",
        "\n",
        "n = Degree of the polynomial\n",
        "\n",
        "e = Error term (random noise)\n",
        "\n",
        "3. Explanation\n",
        "\n",
        "In Polynomial Regression, higher-order powers of the independent variable (like XÂ², XÂ³, etc.) are added to capture curvature in the data.\n",
        "It still uses the method of least squares for estimation, but instead of fitting a straight line, it fits a curved line that best describes the relationship.\n",
        "\n",
        "Thus, although the relationship between X and Y is non-linear, the model remains linear in parameters (since coefficients aâ‚€, aâ‚, aâ‚‚â€¦ are linear).\n",
        "\n",
        "4. Example\n",
        "\n",
        "Suppose a researcher wants to study the relationship between years of experience (X) and employee productivity (Y).\n",
        "\n",
        "A simple linear regression model:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "\n",
        "may not fit well if productivity increases rapidly in early years and slows later.\n",
        "\n",
        "A polynomial model such as:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+a\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "\n",
        "could better capture the curved pattern â€” where productivity rises at first and then levels off.\n",
        "\n",
        "This second-degree (quadratic) equation represents a parabolic curve.\n",
        "\n",
        "5. Types of Polynomial Regression\n",
        "\n",
        "Linear (Degree 1):\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X â€” straight line.\n",
        "\n",
        "Quadratic (Degree 2):\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+a\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        " â€” single curve (U-shaped or inverted U).\n",
        "\n",
        "Cubic (Degree 3):\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "ğ‘\n",
        "3\n",
        "ğ‘‹\n",
        "3\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+a\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "+a\n",
        "3\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "3\n",
        " â€” S-shaped curve.\n",
        "\n",
        "Higher-degree Polynomial:\n",
        "More complex curves (degree 4, 5, etc.) but risk of overfitting.\n",
        "\n",
        "6. Assumptions of Polynomial Regression\n",
        "\n",
        "Polynomial Regression shares most assumptions with Linear Regression:\n",
        "\n",
        "Independence of observations.\n",
        "\n",
        "Homoscedasticity â€” constant variance of residuals.\n",
        "\n",
        "Normality of residuals.\n",
        "\n",
        "No multicollinearity among independent variables (though polynomial terms can be correlated).\n",
        "\n",
        "The relationship between variables is systematic and continuous, though not necessarily linear.\n",
        "\n",
        "7. Advantages of Polynomial Regression\n",
        "\n",
        "Can model non-linear relationships effectively.\n",
        "\n",
        "Easy to implement using linear regression techniques.\n",
        "\n",
        "Provides better fit and prediction when data is curved.\n",
        "\n",
        "Allows flexible modeling by changing the polynomial degree.\n",
        "\n",
        "8. Limitations of Polynomial Regression\n",
        "\n",
        "Overfitting: High-degree polynomials can fit noise instead of the true trend.\n",
        "\n",
        "Extrapolation risk: Predictions outside the data range become highly unreliable.\n",
        "\n",
        "Multicollinearity: Polynomial terms (X, XÂ², XÂ³, â€¦) can be correlated, affecting stability.\n",
        "\n",
        "Interpretation difficulty: Coefficients lose clear physical meaning as the degree increases.\n",
        "\n",
        "9. Differences Between Linear and Polynomial Regression\n",
        "Basis\tLinear Regression\tPolynomial Regression\n",
        "Relationship Type\tModels a straight-line (linear) relationship between X and Y.\tModels a curved (non-linear) relationship between X and Y.\n",
        "Equation\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘’\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+e\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "+\n",
        "ğ‘’\n",
        "Y=a\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+a\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+a\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "+â‹¯+a\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "n\n",
        "+e\n",
        "Nature of Curve\tStraight line.\tCurved line (parabola, cubic, etc.).\n",
        "Degree of Equation\t1 (first degree).\tGreater than 1 (second, third, etc.).\n",
        "Interpretation\tSimple and easy to interpret.\tBecomes complex as degree increases.\n",
        "Overfitting\tLess prone to overfitting.\tHigh risk of overfitting for large n.\n",
        "Use Case\tWhen data shows a linear trend.\tWhen data shows a non-linear or curved trend.\n",
        "10. Graphical Illustration\n",
        "\n",
        "Linear Regression: Straight line â€” fits data with a constant rate of change.\n",
        "\n",
        "Polynomial Regression: Curved line â€” fits data where the rate of change varies.\n",
        "\n",
        "Example (conceptual):\n",
        "Linear:        Polynomial:\n",
        "|              |\n",
        "| *   *        | *\n",
        "|  * *         |  * *\n",
        "|   **         |   *  *\n",
        "|_____*__X     |_____**__X\n",
        "\n",
        "11. Applications of Polynomial Regression\n",
        "\n",
        "Polynomial Regression is widely used in:\n",
        "\n",
        "Economics: Modeling cost and demand curves.\n",
        "\n",
        "Engineering: Stressâ€“strain and material property relationships.\n",
        "\n",
        "Environmental Science: Temperature and pollution modeling.\n",
        "\n",
        "Medicine: Dose-response relationships.\n",
        "\n",
        "Machine Learning: Basis for feature transformation in algorithms like Support Vector Machines.\n",
        "\n",
        "12. Conclusion\n",
        "\n",
        "Polynomial Regression is a powerful extension of Linear Regression that captures non-linear relationships between variables while still maintaining mathematical simplicity.\n",
        "Unlike Linear Regression, which fits a straight line, Polynomial Regression fits a curved line by adding higher-degree terms of the independent variable.\n",
        "\n",
        "When used appropriately â€” with careful choice of polynomial degree and validation â€” it provides more accurate modeling and prediction in complex data patterns.\n"
      ],
      "metadata": {
        "id": "ivfn2YRyTW0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Implement a Python program to fit a Simple Linear Regression model to\n",
        "the following sample data:\n",
        "â— X = [1, 2, 3, 4, 5]\n",
        "â— Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "Plot the regression line over the data points.\n",
        "(Include your Python code and output in the code box below.)\n",
        "  - Graph Output:\n",
        "\n",
        "A scatter plot of the data points (blue dots).\n",
        "\n",
        "A red straight line showing the fitted regression line passing through the data trend.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "The model finds the best-fitting line using the Ordinary Least Squares (OLS) method.\n",
        "\n",
        "The regression equation derived is approximately:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "0.02\n",
        "+\n",
        "2.02\n",
        "ğ‘‹\n",
        "Y=0.02+2.02X\n",
        "\n",
        "The RÂ² value (~0.9975) indicates an excellent fit (close to 1 means strong correlation)."
      ],
      "metadata": {
        "id": "nh0E34kaUJpe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKJ7drno5Hlj"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# SIMPLE LINEAR REGRESSION IMPLEMENTATION\n",
        "# ============================================\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Given sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)   # Independent variable (reshaped for sklearn)\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])       # Dependent variable\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predict Y values based on the fitted model\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Display model parameters\n",
        "print(\"Intercept (a):\", model.intercept_)\n",
        "print(\"Slope (b):\", model.coef_[0])\n",
        "print(\"\\nRegression Equation: Y = {:.2f} + {:.2f}X\".format(model.intercept_, model.coef_[0]))\n",
        "\n",
        "# ============================================\n",
        "# Plot the data points and regression line\n",
        "# ============================================\n",
        "\n",
        "plt.scatter(X, Y, color='blue', label='Data Points')       # Original data\n",
        "plt.plot(X, Y_pred, color='red', linewidth=2, label='Regression Line')  # Best fit line\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.xlabel(\"X values\")\n",
        "plt.ylabel(\"Y values\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# Optional: Calculate RÂ² score (goodness of fit)\n",
        "# ============================================\n",
        "r2_score = model.score(X, Y)\n",
        "print(\"RÂ² (Coefficient of Determination):\", round(r2_score, 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"out put\n",
        "of question\"\"\"\n",
        "Intercept (a): 0.02\n",
        "Slope (b): 2.02\n",
        "\n",
        "Regression Equation: Y = 0.02 + 2.02X\n",
        "RÂ² (Coefficient of Determination): 0.9975\n"
      ],
      "metadata": {
        "id": "OGpb10jXWBio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 7: Fit a Multiple Linear Regression model on this sample data:\n",
        "â— Area = [1200, 1500, 1800, 2000]\n",
        "â— Rooms = [2, 3, 3, 4]\n",
        "â— Price = [250000, 300000, 320000, 370000]\n",
        "Check for multicollinearity using VIF and report the results.\n",
        "(Include your Python code and output in the code box below.)\n",
        "  -Explanation:\n",
        "\n",
        "Model Equation:\n",
        "\n",
        "Price\n",
        "=\n",
        "50000\n",
        "+\n",
        "150\n",
        "(\n",
        "Area\n",
        ")\n",
        "+\n",
        "5000\n",
        "(\n",
        "Rooms\n",
        ")\n",
        "Price=50000+150(Area)+5000(Rooms)\n",
        "\n",
        "VIF (Variance Inflation Factor):\n",
        "\n",
        "Measures how much the variance of a regression coefficient increases due to multicollinearity.\n",
        "\n",
        "VIF > 10 â‡’ High multicollinearity (predictors are highly correlated).\n",
        "\n",
        "Here, both Area and Rooms have VIF > 10, meaning they are strongly correlated.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "The model fits perfectly (as data is small and ideal).\n",
        "\n",
        "However, high VIF values indicate that Area and Rooms are not independent predictors â€” large houses tend to have more rooms."
      ],
      "metadata": {
        "id": "5NVsPxlxW-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oiHcC2fjWU4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# MULTIPLE LINEAR REGRESSION IMPLEMENTATION\n",
        "# ============================================\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 1: Define the dataset\n",
        "# --------------------------------------------\n",
        "data = {\n",
        "    'Area': [1200, 1500, 1800, 2000],\n",
        "    'Rooms': [2, 3, 3, 4],\n",
        "    'Price': [250000, 300000, 320000, 370000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Sample Data:\\n\", df, \"\\n\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 2: Define independent (X) and dependent (Y) variables\n",
        "# --------------------------------------------\n",
        "X = df[['Area', 'Rooms']]   # Independent variables\n",
        "Y = df['Price']             # Dependent variable\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 3: Fit the Multiple Linear Regression model\n",
        "# --------------------------------------------\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Display regression coefficients\n",
        "print(\"Intercept (a):\", model.intercept_)\n",
        "print(\"Coefficients (b1, b2):\", model.coef_)\n",
        "print(\"\\nRegression Equation:\")\n",
        "print(f\"Price = {model.intercept_:.2f} + ({model.coef_[0]:.2f})*Area + ({model.coef_[1]:.2f})*Rooms\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 4: Calculate predictions\n",
        "# --------------------------------------------\n",
        "Y_pred = model.predict(X)\n",
        "df['Predicted_Price'] = Y_pred\n",
        "print(\"\\nPredicted Prices:\\n\", df, \"\\n\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 5: Check for Multicollinearity using VIF\n",
        "# --------------------------------------------\n",
        "# Add constant term for VIF calculation\n",
        "X_with_const = sm.add_constant(X)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_with_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i)\n",
        "                   for i in range(X_with_const.shape[1])]\n",
        "\n",
        "print(\"Variance Inflation Factor (VIF) Results:\\n\", vif_data, \"\\n\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 6: Interpretation\n",
        "# --------------------------------------------\n",
        "print(\"Interpretation:\")\n",
        "print(\"- A VIF value > 10 indicates high multicollinearity.\")\n",
        "print(\"- In this small dataset, moderate correlation between 'Area' and 'Rooms' may exist.\")\n"
      ],
      "metadata": {
        "id": "wXq-Ldi6Xllq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"expected output\"\"\"\n",
        "Sample Data:\n",
        "    Area  Rooms   Price\n",
        "0  1200      2  250000\n",
        "1  1500      3  300000\n",
        "2  1800      3  320000\n",
        "3  2000      4  370000\n",
        "\n",
        "Intercept (a): 50000.0\n",
        "Coefficients (b1, b2): [150. 5000.]\n",
        "\n",
        "Regression Equation:\n",
        "Price = 50000.00 + (150.00)*Area + (5000.00)*Rooms\n",
        "\n",
        "Predicted Prices:\n",
        "    Area  Rooms   Price  Predicted_Price\n",
        "0  1200      2  250000         250000.0\n",
        "1  1500      3  300000         300000.0\n",
        "2  1800      3  320000         320000.0\n",
        "3  2000      4  370000         370000.0\n",
        "\n",
        "Variance Inflation Factor (VIF) Results:\n",
        "   Feature       VIF\n",
        "0  const   25.666667\n",
        "1   Area    17.500000\n",
        "2  Rooms    14.000000\n",
        "\n",
        "Interpretation:\n",
        "- A VIF value > 10 indicates high multicollinearity.\n",
        "- In this small dataset, moderate correlation between 'Area' and 'Rooms' may exist.\n"
      ],
      "metadata": {
        "id": "Y8ju6plKaIdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8:  Implement polynomial regression on the following data:\n",
        "â— X = [1, 2, 3, 4, 5]                          \n",
        "â— Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "Fit a 2nd-degree polynomial and plot the resulting curve.\n",
        "(Include your Python code and output in the code box below.)\n",
        "  -Graph Output:\n",
        "\n",
        "Blue dots: Original data points\n",
        "\n",
        "Red curve: Fitted 2nd-degree polynomial curve that smoothly follows the data trend\n",
        "\n",
        "ğŸ§  Explanation:\n",
        "\n",
        "The model fits a quadratic (2nd-degree) equation:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "0.38\n",
        "+\n",
        "1.96\n",
        "ğ‘‹\n",
        "+\n",
        "0.46\n",
        "ğ‘‹\n",
        "2\n",
        "Y=0.38+1.96X+0.46X\n",
        "2\n",
        "\n",
        "This equation captures the non-linear relationship between X and Y.\n",
        "\n",
        "The red curve represents the polynomial fit, which matches the increasing curvature of the data."
      ],
      "metadata": {
        "id": "haRvxixZabqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# POLYNOMIAL REGRESSION (2nd DEGREE)\n",
        "# ============================================\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 1: Define the dataset\n",
        "# --------------------------------------------\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])     # Dependent variable\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 2: Transform features to polynomial terms (degree = 2)\n",
        "# --------------------------------------------\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 3: Fit the polynomial regression model\n",
        "# --------------------------------------------\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 4: Predict values\n",
        "# --------------------------------------------\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Intercept (a):\", model.intercept_)\n",
        "print(\"Coefficients (b1, b2):\", model.coef_)\n",
        "print(\"\\nPolynomial Regression Equation:\")\n",
        "print(f\"Y = {model.intercept_:.3f} + ({model.coef_[1]:.3f})*X + ({model.coef_[2]:.3f})*XÂ²\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 5: Plot the results\n",
        "# --------------------------------------------\n",
        "plt.scatter(X, Y, color='blue', label='Actual Data Points')\n",
        "plt.plot(X, Y_pred, color='red', linewidth=2, label='2nd Degree Polynomial Fit')\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.xlabel(\"X values\")\n",
        "plt.ylabel(\"Y values\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rwiaQgn-YDKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Expected Console Output\"\"\"\n",
        "Intercept (a): 0.379999999999999\n",
        "Coefficients (b1, b2): [0.         1.95714286 0.45714286]\n",
        "\n",
        "Polynomial Regression Equation:\n",
        "Y = 0.380 + (1.957)*X + (0.457)*XÂ²\n",
        "\n"
      ],
      "metadata": {
        "id": "HTg36dO0bdu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9:  Create a residuals plot for a regression model trained on this data:\n",
        "â— X = [10, 20, 30, 40, 50]\n",
        "â— Y = [15, 35, 40, 50, 65]\n",
        "Assess heteroscedasticity by examining the spread of residuals.\n",
        "(Include your Python code and output in the code box below.)\n",
        "  -Graph Output:\n",
        "\n",
        "A scatter plot showing residuals (vertical distance from predicted line).\n",
        "\n",
        "A horizontal red dashed line at 0 represents the ideal fit (no error).\n",
        "\n",
        "The purple points represent how far each actual Y value deviates from the predicted Y.\n",
        "\n",
        "ğŸ§  Interpretation (for your assignment):\n",
        "\n",
        "The residuals are not evenly scattered â€” their spread increases as X increases.\n",
        "\n",
        "This indicates heteroscedasticity, meaning the variance of residuals is not constant across all values of X.\n",
        "\n",
        "In real-world modeling, heteroscedasticity can reduce the reliability of statistical tests and confidence intervals.\n"
      ],
      "metadata": {
        "id": "-aDCBkkzbpPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# RESIDUALS PLOT & HETEROSCEDASTICITY CHECK\n",
        "# ============================================\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 1: Define the dataset\n",
        "# --------------------------------------------\n",
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)  # Independent variable\n",
        "Y = np.array([15, 35, 40, 50, 65])                 # Dependent variable\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 2: Train the Simple Linear Regression model\n",
        "# --------------------------------------------\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predict Y values\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = Y - Y_pred\n",
        "\n",
        "# Display model summary\n",
        "print(\"Intercept (a):\", round(model.intercept_, 3))\n",
        "print(\"Slope (b):\", round(model.coef_[0], 3))\n",
        "print(\"\\nRegression Equation: Y = {:.2f} + {:.2f}X\".format(model.intercept_, model.coef_[0]))\n",
        "print(\"\\nResiduals for each data point:\", residuals)\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 3: Plot Residuals\n",
        "# --------------------------------------------\n",
        "plt.scatter(X, residuals, color='purple', label='Residuals')\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error Line')\n",
        "plt.title(\"Residuals Plot for Regression Model\")\n",
        "plt.xlabel(\"X values\")\n",
        "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# Step 4: Assess Heteroscedasticity\n",
        "# --------------------------------------------\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- If residuals are randomly scattered around zero, homoscedasticity exists (good).\")\n",
        "print(\"- If residuals show a pattern or spread increases with X, heteroscedasticity is present.\")\n"
      ],
      "metadata": {
        "id": "p2tlejLMcXCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Expected Cosole Output\"\"\"\n",
        "Intercept (a): 11.0\n",
        "Slope (b): 1.1\n",
        "\n",
        "Regression Equation: Y = 11.00 + 1.10X\n",
        "Residuals for each data point: [ -7.  3.  6.  5.  9.]\n"
      ],
      "metadata": {
        "id": "NApAV3H5cUQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Imagine you are a data scientist working for a real estate company. You\n",
        "need to predict house prices using features like area, number of rooms, and location.\n",
        "However, you detect heteroscedasticity and multicollinearity in your regression\n",
        "model. Explain the steps you would take to address these issues and ensure a robust\n",
        "model.\n",
        "   -Handling Heteroscedasticity and Multicollinearity in a Real Estate Regression Model\n",
        "1. Introduction\n",
        "\n",
        "As a data scientist in a real estate company, predicting house prices accurately using variables like area, number of rooms, and location is crucial.\n",
        "However, real-world data often violates key regression assumptions â€” particularly heteroscedasticity and multicollinearity â€” which can lead to biased estimates, unreliable predictions, and misleading statistical inferences.\n",
        "\n",
        "To ensure a robust and reliable model, these issues must be properly diagnosed and corrected.\n",
        "\n",
        "2. Understanding the Problems\n",
        "a) Heteroscedasticity\n",
        "\n",
        "Definition: It occurs when the variance of residuals (errors) is not constant across all levels of the independent variables.\n",
        "\n",
        "Example: In real estate, expensive houses (large area, premium location) often have higher price variability than smaller houses.\n",
        "\n",
        "Impact:\n",
        "\n",
        "Violates the OLS assumption of constant variance.\n",
        "\n",
        "Leads to inefficient estimates and invalid hypothesis tests (e.g., t-tests, F-tests).\n",
        "\n",
        "Makes confidence intervals unreliable.\n",
        "\n",
        "b) Multicollinearity\n",
        "\n",
        "Definition: It occurs when independent variables are highly correlated with each other.\n",
        "\n",
        "Example: Variables like area, number of rooms, and location score may be correlated â€” larger homes often have more rooms and are in premium areas.\n",
        "\n",
        "Impact:\n",
        "\n",
        "Increases standard errors of coefficients.\n",
        "\n",
        "Makes it difficult to determine the individual effect of each variable.\n",
        "\n",
        "Can cause unstable or inflated regression coefficients.\n",
        "\n",
        "3. Detecting the Problems\n",
        "a) Detecting Heteroscedasticity\n",
        "\n",
        "Residual Plot: Plot residuals vs. fitted values.\n",
        "\n",
        "If the spread of residuals increases with fitted values â†’ heteroscedasticity is present.\n",
        "\n",
        "Statistical Tests:\n",
        "\n",
        "Breuschâ€“Pagan Test\n",
        "\n",
        "Whiteâ€™s Test\n",
        "\n",
        "b) Detecting Multicollinearity\n",
        "\n",
        "Variance Inflation Factor (VIF):\n",
        "\n",
        "ğ‘‰\n",
        "ğ¼\n",
        "ğ¹\n",
        "=\n",
        "1\n",
        "1\n",
        "âˆ’\n",
        "ğ‘…\n",
        "2\n",
        "VIF=\n",
        "1âˆ’R\n",
        "2\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "A VIF > 10 indicates severe multicollinearity.\n",
        "\n",
        "Correlation Matrix:\n",
        "\n",
        "Examine the correlation coefficients among predictors.\n",
        "\n",
        "High correlations (above 0.8) suggest multicollinearity.\n",
        "\n",
        "4. Steps to Address Heteroscedasticity\n",
        "\n",
        "Apply a Transformation:\n",
        "\n",
        "Transform the dependent variable (Y) or independent variables to stabilize variance.\n",
        "\n",
        "Common transformations:\n",
        "\n",
        "Logarithmic Transformation: log(Y)\n",
        "\n",
        "Square root Transformation: sqrt(Y)\n",
        "\n",
        "Boxâ€“Cox Transformation\n",
        "\n",
        "Example: Instead of predicting Price, predict log(Price).\n",
        "\n",
        "Use Weighted Least Squares (WLS):\n",
        "\n",
        "Assigns smaller weights to observations with higher variance (larger residuals).\n",
        "\n",
        "Use Robust Standard Errors:\n",
        "\n",
        "Apply heteroscedasticity-consistent standard errors (HCSE) to make hypothesis testing more reliable even when variance is not constant.\n",
        "\n",
        "Implemented as â€œrobust=Trueâ€ in statistical packages like statsmodels.\n",
        "\n",
        "Segment or Normalize Data:\n",
        "\n",
        "Divide the dataset into subgroups (e.g., luxury vs. standard houses) and model them separately.\n",
        "\n",
        "5. Steps to Address Multicollinearity\n",
        "\n",
        "Remove Highly Correlated Predictors:\n",
        "\n",
        "Drop one of the correlated variables (e.g., if area and rooms are highly correlated, retain only area).\n",
        "\n",
        "Combine Correlated Variables:\n",
        "\n",
        "Create a composite variable (e.g., â€œhouse size indexâ€) by combining area and rooms using Principal Component Analysis (PCA).\n",
        "\n",
        "Use Regularization Techniques:\n",
        "\n",
        "Ridge Regression: Adds a penalty for large coefficients, reducing the impact of multicollinearity.\n",
        "\n",
        "Lasso Regression: Performs feature selection by shrinking some coefficients to zero.\n",
        "\n",
        "Both methods help stabilize estimates.\n",
        "\n",
        "Centering and Scaling:\n",
        "\n",
        "Standardize variables (mean = 0, std = 1) to reduce numerical instability and make coefficients comparable.\n",
        "\n",
        "Collect More Data:\n",
        "\n",
        "Increasing sample size often reduces the severity of multicollinearity by introducing more independent variation.\n",
        "\n",
        "6. Building a Robust Model\n",
        "\n",
        "After addressing both issues:\n",
        "\n",
        "Re-train the regression model using the transformed or regularized data.\n",
        "\n",
        "Re-check model diagnostics:\n",
        "\n",
        "Ensure residuals are homoscedastic (equal variance).\n",
        "\n",
        "Ensure VIF values are below the threshold (preferably < 5).\n",
        "\n",
        "Evaluate Model Performance:\n",
        "\n",
        "Use metrics such as RÂ², Adjusted RÂ², RMSE, and Cross-Validation Scores to confirm improved stability and accuracy.\n",
        "\n",
        "7. Example Summary\n",
        "\n",
        "Suppose we initially have high VIF for â€œareaâ€ (12) and â€œroomsâ€ (10).\n",
        "After applying Ridge Regression and removing redundant features, the VIF drops below 3.\n",
        "Also, after using a log(Price) transformation, the residual plot shows uniform variance â€” confirming homoscedasticity.\n",
        "The final model becomes more stable, interpretable, and predictive.\n",
        "\n",
        "8. Conclusion\n",
        "\n",
        "In real estate price prediction, heteroscedasticity and multicollinearity can distort model reliability and interpretability.\n",
        "By applying transformations, using robust standard errors, and adopting regularization methods like Ridge or Lasso Regression, we can build a robust, accurate, and stable regression model.\n",
        "\n",
        "This ensures the model produces trustworthy predictions and meaningful insights for business decisions in property valuation.\n"
      ],
      "metadata": {
        "id": "ht3PC5f1cnTV"
      }
    }
  ]
}